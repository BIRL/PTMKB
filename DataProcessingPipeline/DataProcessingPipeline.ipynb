{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install all libraries here\n",
    "%pip install numpy requests pandas bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for this workflow to run\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import zipfile\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all experimental PTM data from dbPTM and download them\n",
    "\n",
    "BASE_URL = 'https://biomics.lab.nycu.edu.tw/dbPTM' # This is for dbPTM 2025 - I wish dbPTM 2022 URL was up as well for empirical comparison\n",
    "\n",
    "response = requests.get(f'{BASE_URL}/download.php')\n",
    "files_to_download = []\n",
    "\n",
    "if response.ok:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if 'experiment' in href and href.endswith('.zip'):\n",
    "            files_to_download.append(f\"{BASE_URL}/{href}\")\n",
    "\n",
    "if not os.path.exists('./dbptm_data'):\n",
    "    os.mkdir('dbptm_data')\n",
    "\n",
    "all_files = []\n",
    "\n",
    "for file in files_to_download:\n",
    "    actual_file = file.split('/')[-1]\n",
    "    with requests.get(file, stream=True) as response:\n",
    "        with open(f\"dbptm_data/{actual_file}\", 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=4096):\n",
    "                f.write(chunk)\n",
    "    all_files.append(actual_file)\n",
    "    print(\"Downloaded\", actual_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract contents from the downloaded .zip files\n",
    "for file in all_files:\n",
    "    with zipfile.ZipFile(f'dbptm_data/{file}', 'r') as z_f:\n",
    "        z_f.extractall(f'dbptm_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all of the files extracted, we start working on it.\n",
    "# You can see these files are just \"FILE\" files.\n",
    "# They're supposed to be \".tsv\" files (tab-separated values).\n",
    "# Parsing isn't particularly difficult, but it's cumbersome to deal with.\n",
    "\n",
    "# We extract all PTMs to deal with\n",
    "PTMS = [file.split('.')[0] for file in all_files]\n",
    "\n",
    "# This function is used to capture the 21-residue sequence window from the entire protein sequence.\n",
    "# Works pretty well (I think)\n",
    "def construct_subsequence(protein: str, site: int, no_stream_aa: int = 10):\n",
    "    # Treat PTM site as 0-based (so subtract 1 for convenience)\n",
    "    start = max(0, site - no_stream_aa)\n",
    "    end = min(len(protein), site + no_stream_aa + 1)\n",
    "    subsequence: str = protein[start:end]\n",
    "    if site < no_stream_aa:\n",
    "        subsequence = ('-' * (no_stream_aa - site)) + subsequence \n",
    "    if site + no_stream_aa > len(protein):\n",
    "        subsequence += ('-' * ((site + no_stream_aa + 1) - len(protein)))\n",
    "    \n",
    "    return subsequence\n",
    "\n",
    "\n",
    "# Run this function for each PTM file to get protein sequences for all proteins in each PTM file which are not given\n",
    "def preprocess(filepath: str):\n",
    "    # Manual label of columns for reference\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        names=[\n",
    "            'ProID',\n",
    "            'Acc#',\n",
    "            'ModSite',\n",
    "            'PTM',\n",
    "            'EvdId',\n",
    "            'Seq'\n",
    "        ],\n",
    "        header=None,\n",
    "        sep='\\t'\n",
    "    )\n",
    "\n",
    "    # Extract all rows which have no sequence windows\n",
    "    na_df = df[df['Seq'].isna()]\n",
    "    if len(na_df):\n",
    "        # If there is at least one row, we use that to process.\n",
    "        print(\"For file\", filepath.split('/')[-1])\n",
    "        # This is a point of failure. Thankfully, no such empty sequence window exists for which no UniProt Accession Number is given\n",
    "        # Also, unique called - can't have the API handle \n",
    "        accession_numbers = na_df['Acc#'].unique().tolist()\n",
    "        for accession_number in accession_numbers:\n",
    "            got_response = True\n",
    "            got_sequence = True\n",
    "            while got_response:\n",
    "                try:\n",
    "                    got_response = False\n",
    "                    # Fetch response from EBI API.\n",
    "                    response = requests.get( # I understand doing this asynchronously would have benefitted majorly, but I was too lazy to handle them\n",
    "                        f'http://www.ebi.ac.uk/proteins/api/proteins/{accession_number}' # Also, thankfully, much less protein sequences to deal with\n",
    "                    )\n",
    "                    print(f\"\\tGot response for {accession_number}.\")\n",
    "                except:\n",
    "                    got_response = True\n",
    "                try:\n",
    "                    # Would have preferred calling \".get()\", but this works too I guess...\n",
    "                    sequence = response.json()['sequence']['sequence']\n",
    "                except:\n",
    "                    got_sequence = False\n",
    "                    sequence = ''\n",
    "                    print(\"No sequence found for\", accession_number)\n",
    "            \n",
    "            # Some processing where we get all recorded mod sites for that protein\n",
    "            positions = na_df[na_df['Acc#'] == accession_number].ModSite.to_list()\n",
    "            for position in positions:\n",
    "                # If the sequence was retrieved, we extract the 21-residue sequence window relative to the mod site.\n",
    "                if got_sequence:\n",
    "                    subsequence = construct_subsequence(sequence, position-1)\n",
    "                else:\n",
    "                    subsequence = ''\n",
    "                df.loc[(df['Acc#'] == accession_number) & (df['ModSite'] == position), 'Seq'] = subsequence\n",
    "\n",
    "        # Going to save the new file as a CSV (maybe not a good idea, but at no point would you actually have commas in dbPTM's data)\n",
    "        df.to_csv(f\"{filepath}.csv\", index=False)\n",
    "    else:\n",
    "        # Just save it as is, no alterations needed for now.\n",
    "        df.to_csv(f\"{filepath}.csv\", index=False)\n",
    "        print(\"Clear for\", filepath.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should have removed zip files, but kept them for restoration purposes\n",
    "all_files = [i.split('.')[0] for i in glob('dbptm_data/*.zip')]\n",
    "for file in all_files:\n",
    "    preprocess(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now in all honesty, this is a pretty stupid function, but there's a reason why it's so absurd\n",
    "# This is all done in the case where the EBI API was unable to fetch the protein sequence from the Accession Number\n",
    "# So we think there are a few possibilities:\n",
    "# 1. First, just query the same accession number using rest.uniprot instead of the EBI API \n",
    "#    If the protein sequence is found, huzzah! Otherwise:\n",
    "# 2. Check why no protein sequence exists\n",
    "# 2.1. In the case of the protein sequence being deleted, fetch the protein's UNIPARC ID and use that to query to ANOTHER URL for the protein sequence - this always works, so no need to hypervise\n",
    "# 2.2. In the case of the protein sequence being demerged, get the first available accession number in the merged list and use that accession number to make a request and get the protein sequence\n",
    "def preprocess_again(filepath: str):\n",
    "    df = pd.read_csv(\n",
    "        filepath\n",
    "    )\n",
    "\n",
    "    na_df = df[df['Seq'].isna()]\n",
    "\n",
    "    if len(na_df):\n",
    "        print(\"Doing for\", filepath.split('/')[-1])\n",
    "        accession_numbers = na_df['Acc#'].unique().tolist()\n",
    "        for accession_number in accession_numbers:\n",
    "            got_response = True\n",
    "            while got_response:\n",
    "                try:\n",
    "                    got_response = False\n",
    "                    response = requests.get(\n",
    "                        f'https://rest.uniprot.org/uniprotkb/{accession_number}',\n",
    "                        headers={\n",
    "                            'Accept': 'application/json'\n",
    "                        }\n",
    "                    )\n",
    "                    print(\"\\tGot response for\", accession_number)\n",
    "                except Exception as e:\n",
    "                    got_response = True\n",
    "            got_sequence = True\n",
    "            try:\n",
    "                response: dict = response.json()\n",
    "                if 'inactiveReason' in response.keys():\n",
    "                    if response['inactiveReason']['inactiveReasonType'] == 'DELETED':\n",
    "                        print(\"\\tWas deleted\")\n",
    "                        uniparc_id = response['extraAttributes']['uniParcId']\n",
    "                        print(uniparc_id)\n",
    "                        got_response = True\n",
    "                        while got_response:\n",
    "                            try:\n",
    "                                got_response = False\n",
    "                                response: dict = requests.get(\n",
    "                                    f'https://rest.uniprot.org/uniparc/{uniparc_id}',\n",
    "                                    headers={\n",
    "                                        'Accept': 'application/json'\n",
    "                                    }\n",
    "                                )\n",
    "                            except Exception as e:\n",
    "                                got_response = True\n",
    "                    elif response['inactiveReason']['inactiveReasonType'] == 'DEMERGED':    \n",
    "                        print(\"\\tWas merged\")\n",
    "                        new_accession_number = response['inactiveReason']['mergeDemergeTo'][0]\n",
    "                        print(new_accession_number)\n",
    "                        got_response = True\n",
    "                        while got_response:\n",
    "                            try:\n",
    "                                got_response = False\n",
    "                                response: dict = requests.get(\n",
    "                                    f'https://rest.uniprot.org/uniprotkb/{new_accession_number}',\n",
    "                                    headers={\n",
    "                                        'Accept': 'application/json'\n",
    "                                    }\n",
    "                                )\n",
    "                            except Exception as e:\n",
    "                                got_response = True\n",
    "                    print(\"Retrieved check\")\n",
    "                    print(response.json()['sequence']['value'])\n",
    "                    print(\"Clearance check\")\n",
    "                    sequence = response.json()['sequence']['value']\n",
    "                else:\n",
    "                    sequence = response['sequence']['value']\n",
    "            except Exception as e:\n",
    "                sequence = ''\n",
    "                got_sequence = False\n",
    "                print(\"\\t\\tNo sequence found for\", accession_number)\n",
    "                \n",
    "            positions = na_df[na_df['Acc#'] == accession_number].ModSite.to_list()\n",
    "            for position in positions:\n",
    "                if got_sequence:\n",
    "                    subsequence = construct_subsequence(sequence, position-1)\n",
    "                else:\n",
    "                    subsequence = ''\n",
    "                df.loc[(df['Acc#'] == accession_number) & (df['ModSite'] == position), 'Seq'] = subsequence\n",
    "        # Overwrite the information\n",
    "        df.to_csv(f\"{filepath}.csv\", index=False)\n",
    "        \n",
    "        return 1\n",
    "    else:\n",
    "        print(\"Nothing for\", filepath.split('/')[-1])\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same deal again\n",
    "all_files = [i for i in glob('dbptm_data/*.csv')]\n",
    "for file in all_files:\n",
    "    preprocess_again(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is actually stupid, don't try this, just edit the above funtion\n",
    "# This just fetches the missing accession numbers\n",
    "def preprocess_again_again(filepath: str):\n",
    "    df = pd.read_csv(\n",
    "        filepath\n",
    "    )\n",
    "\n",
    "    na_df = df[df['Acc#'].isna()]\n",
    "\n",
    "    if len(na_df):\n",
    "        print(\"Doing for\", filepath.split('/')[-1])\n",
    "        accession_numbers = na_df['ProID'].unique().tolist()\n",
    "        for accession_number in accession_numbers:\n",
    "            got_response = True\n",
    "            while got_response:\n",
    "                try:\n",
    "                    got_response = False\n",
    "                    response = requests.get(\n",
    "                        f'https://rest.uniprot.org/uniprotkb/{accession_number}',\n",
    "                        headers={\n",
    "                            'Accept': 'application/json'\n",
    "                        }\n",
    "                    )\n",
    "                    print(\"\\tGot response for\", accession_number)\n",
    "                except Exception as e:\n",
    "                    got_response = True\n",
    "            got_sequence = True\n",
    "            try:\n",
    "                response: dict = response.json()\n",
    "                if 'inactiveReason' in response.keys():\n",
    "                    if response['inactiveReason']['inactiveReasonType'] == 'DELETED':\n",
    "                        print(\"\\tWas deleted\")\n",
    "                        uniparc_id = response['extraAttributes']['uniParcId']\n",
    "                        print(uniparc_id)\n",
    "                        got_response = True\n",
    "                        while got_response:\n",
    "                            try:\n",
    "                                got_response = False\n",
    "                                response: dict = requests.get(\n",
    "                                    f'https://rest.uniprot.org/uniparc/{uniparc_id}',\n",
    "                                    headers={\n",
    "                                        'Accept': 'application/json'\n",
    "                                    }\n",
    "                                )\n",
    "                            except Exception as e:\n",
    "                                got_response = True\n",
    "                    elif response['inactiveReason']['inactiveReasonType'] == 'DEMERGED':    \n",
    "                        print(\"\\tWas merged\")\n",
    "                        new_accession_number = response['inactiveReason']['mergeDemergeTo'][0]\n",
    "                        print(new_accession_number)\n",
    "                        got_response = True\n",
    "                        while got_response:\n",
    "                            try:\n",
    "                                got_response = False\n",
    "                                response: dict = requests.get(\n",
    "                                    f'https://rest.uniprot.org/uniprotkb/{new_accession_number}',\n",
    "                                    headers={\n",
    "                                        'Accept': 'application/json'\n",
    "                                    }\n",
    "                                )\n",
    "                            except Exception as e:\n",
    "                                got_response = True\n",
    "                    print(\"Retrieved check\")\n",
    "                    print(response.json()['sequence']['value'])\n",
    "                    print(\"Clearance check\")\n",
    "                    sequence = response.json()['sequence']['value']\n",
    "                else:\n",
    "                    sequence = response['sequence']['value']\n",
    "            except Exception as e:\n",
    "                sequence = ''\n",
    "                got_sequence = False\n",
    "                print(\"\\t\\tNo sequence found for\", accession_number)\n",
    "                \n",
    "            positions = na_df[na_df['Acc#'] == accession_number].ModSite.to_list()\n",
    "            for position in positions:\n",
    "                if got_sequence:\n",
    "                    subsequence = construct_subsequence(sequence, position-1)\n",
    "                else:\n",
    "                    subsequence = ''\n",
    "                df.loc[(df['Acc#'] == accession_number) & (df['ModSite'] == position), 'Seq'] = subsequence\n",
    "        # Overwrite the information\n",
    "        df.to_csv(f\"{filepath}.csv\", index=False)\n",
    "        \n",
    "        return 1\n",
    "    else:\n",
    "        print(\"Nothing for\", filepath.split('/')[-1])\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once more\n",
    "all_files = [i for i in glob('dbptm_data/*.csv')]\n",
    "for file in all_files:\n",
    "    preprocess_again(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all of that done, we can finally move on to the next step\n",
    "import typing\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Make a list of residues/amino acids\n",
    "AA = \"A C D E F G H I K L M N P Q R S T V W Y\".split(' ')\n",
    "\n",
    "# Now we have this function that constructs positional frequency matrices.\n",
    "# You can calculate by frequency, by natural log frequency, or any of the base 2 or base 10 frequencies.\n",
    "# It's defaulted to 'freq' but you can change it below\n",
    "# Also this function takes in all filenames instead of a single one\n",
    "# Thought it better to do this way\n",
    "def calculate_values(filenames: list[str]):\n",
    "    # For each PTM\n",
    "    for filename in filenames:\n",
    "        print(\"For\", filename)\n",
    "        df = pd.read_csv(\n",
    "            filename,\n",
    "        )\n",
    "        df['mid'] = df['Seq'].str[10]\n",
    "        sub_dfs = [group for _, group in df.groupby('mid')]\n",
    "        # For each residue/amino acid\n",
    "        for sub_df in sub_dfs:\n",
    "            # Sanity check - the dataset was pretty unclean\n",
    "            # Get all sequences where the mod site is said amino acid\n",
    "            sequences = sub_df['Seq'].to_list()\n",
    "            # Transpose them\n",
    "            transposed = list(map(''.join, zip(*sequences)))\n",
    "            # Construct relative positions\n",
    "            pos = [f\"+{i}\" if i > 0 else f\"{i}\" for i in range(-10, 11)]\n",
    "            # Create a table based on relative position per amino acid\n",
    "            table = {i: seq for i, seq in zip(pos, transposed)}\n",
    "\n",
    "            def log_odds(sequences: dict, method: typing.Literal['freq', 'log-e', 'log10', 'log2'] = 'freq') -> dict[dict[str, float]]:\n",
    "                table = dict.fromkeys(sequences.keys(), 0)\n",
    "\n",
    "                for position, sequence in sequences.items():\n",
    "                    log_odds_table = dict.fromkeys(AA, 0)\n",
    "                    for aa in AA:\n",
    "                        # Formula is count of an amino acid 'aa' over count of all amino acids in the transposed sequence (or basically length of a transposed array of protein sequences)\n",
    "                        total = len(sequence)\n",
    "                        count = sequence.count(aa)\n",
    "                        log_odds_table[aa] = count / total # For now, let's keep it simple count_a / count\n",
    "                        # And then apply NumPy-compatible logarithms\n",
    "                        if method == 'log-e':\n",
    "                            log_odds_table[aa] = np.log(log_odds_table[aa])\n",
    "                        elif method == 'log10':\n",
    "                            log_odds_table[aa] = np.log10(log_odds_table[aa])\n",
    "                        elif method == 'log2':\n",
    "                            log_odds_table[aa] = np.log2(log_odds_table[aa])\n",
    "                        \n",
    "                        if log_odds_table[aa] == float('-inf'):\n",
    "                            log_odds_table[aa] = '-inf'\n",
    "\n",
    "                    table[position] = log_odds_table\n",
    "\n",
    "                return table\n",
    "\n",
    "            method = 'freq' # CHANGE METHOD HERE\n",
    "            # Calculate matrices for the table\n",
    "            table = log_odds(table, method)\n",
    "\n",
    "            print(\"\\tCalculated against\", sub_df.iloc[0]['mid'])\n",
    "            if not os.path.exists(f\"tables/{filename.split('.')[0]}\"):\n",
    "                os.mkdir(f\"tables/{filename.split('.')[0]}\")\n",
    "            if not os.path.exists(f\"tables/{filename.split('.')[0]}/{method}\"):\n",
    "                os.mkdir(f\"tables/{filename.split('.')[0]}/{method}\")\n",
    "            with open(f\"tables/{filename.split('.')[0]}/{method}/{sub_df.iloc[0]['mid']}.json\", 'w') as f:\n",
    "                json.dump(table, f, indent=2)\n",
    "            # save_table(pd.DataFrame(table), filename)\n",
    "        print(\"Calculated for\", filename.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, NOW once more - for real this time\n",
    "all_files = [i for i in glob('dbptm_data/*.csv')]\n",
    "calculate_values(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And there you have it! The tables have been constructed using a clean and processed version of dbPTM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
